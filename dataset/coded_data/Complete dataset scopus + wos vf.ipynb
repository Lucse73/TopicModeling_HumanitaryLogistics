{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = pd.read_excel('savedrecs (1).xls')\n",
    "w2 = pd.read_excel('savedrecs (2).xls')\n",
    "w3 = pd.read_excel('savedrecs (3).xls')\n",
    "w4 = pd.read_excel('savedrecs (4).xls')\n",
    "w5 = pd.read_excel('savedrecs (5).xls')\n",
    "w6 = pd.read_excel('savedrecs (6).xls')\n",
    "w7 = pd.read_excel('savedrecs (7).xls')\n",
    "w8 = pd.read_excel('savedrecs (8).xls')\n",
    "w9 = pd.read_excel('savedrecs (9).xls')\n",
    "w10 = pd.read_excel('savedrecs (10).xls')\n",
    "w11 = pd.read_excel('savedrecs (11).xls')\n",
    "w12 = pd.read_excel('savedrecs (12).xls')\n",
    "w13 = pd.read_excel('savedrecs (13).xls')\n",
    "w14 = pd.read_excel('savedrecs (14).xls')\n",
    "w15 = pd.read_excel('savedrecs (15).xls')\n",
    "w16 = pd.read_excel('savedrecs (16).xls')\n",
    "w17 = pd.read_excel('savedrecs (17).xls')\n",
    "w18 = pd.read_excel('savedrecs (18).xls')\n",
    "w19 = pd.read_excel('savedrecs (19).xls')\n",
    "w20 = pd.read_excel('savedrecs (20).xls')\n",
    "w21 = pd.read_excel('savedrecs (21).xls')\n",
    "w22 = pd.read_excel('savedrecs (22).xls')\n",
    "w23 = pd.read_excel('savedrecs (23).xls')\n",
    "w24 = pd.read_excel('savedrecs (24).xls')\n",
    "w25 = pd.read_excel('savedrecs (25).xls')\n",
    "w26 = pd.read_excel('savedrecs (26).xls')\n",
    "w27 = pd.read_excel('savedrecs (27).xls')\n",
    "w28 = pd.read_excel('savedrecs (28).xls')\n",
    "w29 = pd.read_excel('savedrecs (29).xls')\n",
    "w30 = pd.read_excel('savedrecs (30).xls')\n",
    "w31 = pd.read_excel('savedrecs (31).xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_wos = [w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15, w16, w17, w18, w19, w20, w21, w22, w23, w24, w25,w26, w27, w28, w29, w30, w31]  # List of the dataframes\n",
    "data_wos = pd.concat(list_wos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30369"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_wos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Publication Type', 'Authors', 'Book Authors', 'Book Editors',\n",
       "       'Book Group Authors', 'Author Full Names', 'Book Author Full Names',\n",
       "       'Group Authors', 'Article Title', 'Source Title', 'Book Series Title',\n",
       "       'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title',\n",
       "       'Conference Date', 'Conference Location', 'Conference Sponsor',\n",
       "       'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract',\n",
       "       'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses',\n",
       "       'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred',\n",
       "       'Funding Text', 'Cited References', 'Cited Reference Count',\n",
       "       'Times Cited, WoS Core', 'Times Cited, All Databases',\n",
       "       '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher',\n",
       "       'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN',\n",
       "       'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date',\n",
       "       'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement',\n",
       "       'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page',\n",
       "       'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date',\n",
       "       'Number of Pages', 'WoS Categories', 'Web of Science Index',\n",
       "       'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations',\n",
       "       'Highly Cited Status', 'Hot Paper Status', 'Date of Export',\n",
       "       'UT (Unique WOS ID)', 'Web of Science Record'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wos = pd.DataFrame()\n",
    "df_wos['DOI'] = data_wos['DOI']\n",
    "df_wos['Article_Title'] = data_wos['Article Title']\n",
    "df_wos['Abstract'] = data_wos['Abstract']\n",
    "df_wos['Author_Keywords'] = data_wos['Author Keywords']\n",
    "df_wos['Author_Full_Names'] = data_wos['Author Full Names']\n",
    "df_wos['Source_Title'] = data_wos['Source Title']\n",
    "df_wos['Publication_Year'] = data_wos['Publication Year']\n",
    "df_wos['DB'] = 'Web of Science' #DB as database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Article_Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Author_Keywords</th>\n",
       "      <th>Author_Full_Names</th>\n",
       "      <th>Source_Title</th>\n",
       "      <th>Publication_Year</th>\n",
       "      <th>DB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1111/j.1745-493X.2012.03267.x</td>\n",
       "      <td>Humanitarian and Disaster Relief Supply Chains...</td>\n",
       "      <td>With an increasing number of disasters disrupt...</td>\n",
       "      <td>humanitarian; disaster; relief planning; respo...</td>\n",
       "      <td>Day, Jamison M.; Melnyk, Steven A.; Larson, Pa...</td>\n",
       "      <td>JOURNAL OF SUPPLY CHAIN MANAGEMENT</td>\n",
       "      <td>2012</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1111/j.0000-0000.2012.01047.x</td>\n",
       "      <td>Disaster-Relief Logistics With Limited Fuel Su...</td>\n",
       "      <td>Disaster-relief logistics consists of providin...</td>\n",
       "      <td>disaster-relief logistics; vehicle routing; fu...</td>\n",
       "      <td>Suzuki, Yoshinori</td>\n",
       "      <td>JOURNAL OF BUSINESS LOGISTICS</td>\n",
       "      <td>2012</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1016/j.ijdrr.2017.10.005</td>\n",
       "      <td>Integrated blood supply chain planning for dis...</td>\n",
       "      <td>This paper proposes a multi-objective mixed in...</td>\n",
       "      <td>Blood supply chain; Disaster relief; Multi-obj...</td>\n",
       "      <td>Samani, Mohammad Reza Ghatreh; Torabi, S. Ali;...</td>\n",
       "      <td>INTERNATIONAL JOURNAL OF DISASTER RISK REDUCTION</td>\n",
       "      <td>2018</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1057/palgrave.jors.2602125</td>\n",
       "      <td>Blackett Memorial Lecture - Humanitarian aid l...</td>\n",
       "      <td>This paper builds on the idea that private sec...</td>\n",
       "      <td>emergency relief operations; humanitarian logi...</td>\n",
       "      <td>Van Wassenhove, LN</td>\n",
       "      <td>JOURNAL OF THE OPERATIONAL RESEARCH SOCIETY</td>\n",
       "      <td>2006</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1111/jbl.12362</td>\n",
       "      <td>Inherent and adaptive resilience of logistics ...</td>\n",
       "      <td>This study aims to investigate the sources of ...</td>\n",
       "      <td>adaptive resilience; disaster; food supply cha...</td>\n",
       "      <td>Umar, Muhammad; Wilson, Mark M. J.</td>\n",
       "      <td>JOURNAL OF BUSINESS LOGISTICS</td>\n",
       "      <td>2024</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                DOI  \\\n",
       "0  10.1111/j.1745-493X.2012.03267.x   \n",
       "1  10.1111/j.0000-0000.2012.01047.x   \n",
       "2       10.1016/j.ijdrr.2017.10.005   \n",
       "3     10.1057/palgrave.jors.2602125   \n",
       "4                 10.1111/jbl.12362   \n",
       "\n",
       "                                       Article_Title  \\\n",
       "0  Humanitarian and Disaster Relief Supply Chains...   \n",
       "1  Disaster-Relief Logistics With Limited Fuel Su...   \n",
       "2  Integrated blood supply chain planning for dis...   \n",
       "3  Blackett Memorial Lecture - Humanitarian aid l...   \n",
       "4  Inherent and adaptive resilience of logistics ...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  With an increasing number of disasters disrupt...   \n",
       "1  Disaster-relief logistics consists of providin...   \n",
       "2  This paper proposes a multi-objective mixed in...   \n",
       "3  This paper builds on the idea that private sec...   \n",
       "4  This study aims to investigate the sources of ...   \n",
       "\n",
       "                                     Author_Keywords  \\\n",
       "0  humanitarian; disaster; relief planning; respo...   \n",
       "1  disaster-relief logistics; vehicle routing; fu...   \n",
       "2  Blood supply chain; Disaster relief; Multi-obj...   \n",
       "3  emergency relief operations; humanitarian logi...   \n",
       "4  adaptive resilience; disaster; food supply cha...   \n",
       "\n",
       "                                   Author_Full_Names  \\\n",
       "0  Day, Jamison M.; Melnyk, Steven A.; Larson, Pa...   \n",
       "1                                  Suzuki, Yoshinori   \n",
       "2  Samani, Mohammad Reza Ghatreh; Torabi, S. Ali;...   \n",
       "3                                 Van Wassenhove, LN   \n",
       "4                 Umar, Muhammad; Wilson, Mark M. J.   \n",
       "\n",
       "                                       Source_Title  Publication_Year  \\\n",
       "0                JOURNAL OF SUPPLY CHAIN MANAGEMENT              2012   \n",
       "1                     JOURNAL OF BUSINESS LOGISTICS              2012   \n",
       "2  INTERNATIONAL JOURNAL OF DISASTER RISK REDUCTION              2018   \n",
       "3       JOURNAL OF THE OPERATIONAL RESEARCH SOCIETY              2006   \n",
       "4                     JOURNAL OF BUSINESS LOGISTICS              2024   \n",
       "\n",
       "               DB  \n",
       "0  Web of Science  \n",
       "1  Web of Science  \n",
       "2  Web of Science  \n",
       "3  Web of Science  \n",
       "4  Web of Science  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wos = df_wos.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wos = df_wos.dropna(subset=['Article_Title', 'Abstract', 'Author_Keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24956, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scopus = pd.read_csv('scopus original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7523"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_scopus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Authors', 'Author full names', 'Author(s) ID', 'Title', 'Year',\n",
       "       'Source title', 'Volume', 'Issue', 'Art. No.', 'Page start', 'Page end',\n",
       "       'Page count', 'Cited by', 'DOI', 'Link', 'Abstract', 'Author Keywords',\n",
       "       'Index Keywords', 'Document Type', 'Publication Stage', 'Open Access',\n",
       "       'Source', 'EID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scopus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scopus = pd.DataFrame()\n",
    "df_scopus['DOI'] = data_scopus['DOI']\n",
    "df_scopus['Article_Title'] = data_scopus['Title']\n",
    "df_scopus['Abstract'] = data_scopus['Abstract']\n",
    "df_scopus['Author_Keywords'] = data_scopus['Author Keywords']\n",
    "df_scopus['Author_Full_Names'] = data_scopus['Authors']\n",
    "df_scopus['Source_Title'] = data_scopus['Source title']\n",
    "df_scopus['Publication_Year'] = data_scopus['Year']\n",
    "df_scopus['DB'] = 'Scopus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7523, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scopus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scopus = df_scopus.dropna(subset=['Article_Title', 'Abstract', 'Author_Keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6142, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scopus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scopus = df_scopus.drop(df_scopus[df_scopus['Abstract'] == '[No abstract available]'].index) \n",
    "df_scopus = df_scopus.drop(df_scopus[df_scopus['Author_Full_Names'] == '[No author available]'].index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6136, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scopus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scopus.to_csv(\"scopus.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wos.to_csv(\"wos.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6136, 8), (24956, 8))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scopus.shape, df_wos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unido = pd.concat([df_scopus, df_wos], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31092, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unido.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates across both df_scopus and df_wos: 4166\n"
     ]
    }
   ],
   "source": [
    "num_duplicados_doi = df_unido['DOI'].duplicated().sum()\n",
    "print(f\"Number of duplicates across both df_scopus and df_wos: {num_duplicados_doi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unido = df_unido.drop_duplicates(subset=['DOI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               DB  Count\n",
      "0          Scopus   5899\n",
      "1  Web of Science  21027\n"
     ]
    }
   ],
   "source": [
    "conteo_por_db = df_unido.groupby('DB').size().reset_index(name='Count')\n",
    "print(conteo_por_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unido.to_csv(\"completo.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiezaNum(ele):\n",
    "    return \"\".join(re.findall(r'[0-9]+',ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1 = np.array([int(limpiezaNum(ele)) for ele in df_scopus['Page_start']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2 = np.array([int(limpiezaNum(ele)) for ele in df_scopus['Page_end']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3011, 3011)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num1),len(num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scopus['Number_of_Pages'] = num2-num1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scopus = df_scopus.drop(['Page_start', 'Page_end'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scopus = df_scopus.reindex(columns=['DOI','Article_Title','Abstract','Author_Keywords','Author_Full_Names','Addresses','Source_Title','Language','Times_Cited_ALL','Publication_Year','Number_of_Pages','Research_Areas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3011, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scopus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wos_scopus = pd.DataFrame()\n",
    "df_wos_scopus = pd.concat([df_wos,df_scopus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18183, 13)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wos_scopus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18183, 13)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wos_scopus = df_wos_scopus.drop_duplicates()\n",
    "df_wos_scopus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18183, 13)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wos_scopus['Author_Keywords'] = df_wos_scopus['Author_Keywords'].astype(str)\n",
    "df_wos_scopus = df_wos_scopus.drop(df_wos_scopus[df_wos_scopus['Abstract'] == '[No abstract available]'].index) \n",
    "# After deleting some rows due to the condition above, we need to restart the count index\n",
    "df_wos_scopus.reset_index(drop=True, inplace=True)\n",
    "df_wos_scopus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wos_scopus.to_excel('df_wos_scopus_vf.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1111/j.1745-493X.2012.03267.x</td>\n",
       "      <td>Humanitarian and Disaster Relief Supply Chains...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1016/j.ejor.2020.10.016</td>\n",
       "      <td>Building disaster preparedness and response ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1057/palgrave.jors.2602125</td>\n",
       "      <td>Blackett Memorial Lecture - Humanitarian aid l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1108/SCM-06-2015-0244</td>\n",
       "      <td>A commentary on agility in humanitarian aid su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1007/s10479-017-2587-z</td>\n",
       "      <td>Reducing the cost of humanitarian operations t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18178</th>\n",
       "      <td>10.1007/s00500-019-04287-7</td>\n",
       "      <td>An optimal redistribution plan considering aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18179</th>\n",
       "      <td>10.1007/s00380-019-01450-w</td>\n",
       "      <td>Predictors of responders for low-dose carperit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18180</th>\n",
       "      <td>10.1007/s10479-019-03246-7</td>\n",
       "      <td>Decentralized beneficiary behavior in humanita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18181</th>\n",
       "      <td>10.1016/j.sapharm.2019.03.146</td>\n",
       "      <td>Exploring the experiences and preparedness of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18182</th>\n",
       "      <td>10.1016/j.ssci.2019.02.014</td>\n",
       "      <td>Application of household disruption data to de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18183 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    DOI  \\\n",
       "0      10.1111/j.1745-493X.2012.03267.x   \n",
       "1            10.1016/j.ejor.2020.10.016   \n",
       "2         10.1057/palgrave.jors.2602125   \n",
       "3              10.1108/SCM-06-2015-0244   \n",
       "4             10.1007/s10479-017-2587-z   \n",
       "...                                 ...   \n",
       "18178        10.1007/s00500-019-04287-7   \n",
       "18179        10.1007/s00380-019-01450-w   \n",
       "18180        10.1007/s10479-019-03246-7   \n",
       "18181     10.1016/j.sapharm.2019.03.146   \n",
       "18182        10.1016/j.ssci.2019.02.014   \n",
       "\n",
       "                                                    Text  \n",
       "0      Humanitarian and Disaster Relief Supply Chains...  \n",
       "1      Building disaster preparedness and response ca...  \n",
       "2      Blackett Memorial Lecture - Humanitarian aid l...  \n",
       "3      A commentary on agility in humanitarian aid su...  \n",
       "4      Reducing the cost of humanitarian operations t...  \n",
       "...                                                  ...  \n",
       "18178  An optimal redistribution plan considering aft...  \n",
       "18179  Predictors of responders for low-dose carperit...  \n",
       "18180  Decentralized beneficiary behavior in humanita...  \n",
       "18181  Exploring the experiences and preparedness of ...  \n",
       "18182  Application of household disruption data to de...  \n",
       "\n",
       "[18183 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = pd.DataFrame()\n",
    "input_data['DOI'] = df_wos_scopus['DOI']\n",
    "input_data['Text'] = df_wos_scopus[['Article_Title', 'Abstract', 'Author_Keywords' ]].agg('-'.join, axis=1)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.to_excel('inputdata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_excel('inputdata.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from unicodedata import normalize\n",
    "import spacy\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstclean(texto):\n",
    "    '''Given a string of text, rejects words with less than 3 characters, numbers, underscores, whitespace, quote marks, and other punctuation.'''\n",
    "    out = []\n",
    "    texto = texto.lower()\n",
    "    tokens =  re.findall(r'[a-zA-Z]+',texto)\n",
    "    for w in tokens:\n",
    "        if (len(w)>3):\n",
    "            out.append(w)\n",
    "    \n",
    "    return \" \".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['Text'] = input_data['Text'].apply(firstclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.to_excel('inputdata2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(stopwords.words(\"english\"))\n",
    "# academic vocab \n",
    "extras = [\"humanitarian\",\"relief\", \"disaster\", \"logistic\", \"supply\", \"chain\", \"operations\", \"response\", \"preparedness\", \"mitigation\", \"recovery\",\"abstract\",\"research\",\"paper\",\"purpose\", \"design\", \"methodology\", \"approach\", \"findings\", \"use\", \"analysis\", \"practical\", \"result\", \"problem\", \"article\", \"implications\", \"originality\", \"value\", \"conclusion\", \"method\", \"result\",\"study\",\"aims\",\"aim\"]\n",
    "# join both lists\n",
    "sw = stopwords + extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'humanitarian',\n",
       " 'relief',\n",
       " 'disaster',\n",
       " 'logistic',\n",
       " 'supply',\n",
       " 'chain',\n",
       " 'operations',\n",
       " 'response',\n",
       " 'preparedness',\n",
       " 'mitigation',\n",
       " 'recovery',\n",
       " 'abstract',\n",
       " 'research',\n",
       " 'paper',\n",
       " 'purpose',\n",
       " 'design',\n",
       " 'methodology',\n",
       " 'approach',\n",
       " 'findings',\n",
       " 'use',\n",
       " 'analysis',\n",
       " 'practical',\n",
       " 'result',\n",
       " 'problem',\n",
       " 'article',\n",
       " 'implications',\n",
       " 'originality',\n",
       " 'value',\n",
       " 'conclusion',\n",
       " 'method',\n",
       " 'result',\n",
       " 'study',\n",
       " 'aims',\n",
       " 'aim']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar(texto):\n",
    "    '''Given a string of text, tokenize the text and check if all characters are alphabets or if there are whitespaces.'''\n",
    "    cad = ''\n",
    "    for c in texto.lower():\n",
    "        if c.isalpha() or c.isspace():\n",
    "            cad += c\n",
    "    words = [w for w in cad.split() if w not in sw]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['Text'] = input_data['Text'].apply(limpiar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1111/j.1745-493X.2012.03267.x</td>\n",
       "      <td>chains matter life death increasing number dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.1016/j.ejor.2020.10.016</td>\n",
       "      <td>building capacity chains using social vulnerab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10.1057/palgrave.jors.2602125</td>\n",
       "      <td>blackett memorial lecture logistics management...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.1108/SCM-06-2015-0244</td>\n",
       "      <td>commentary agility chains provide commentary o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10.1007/s10479-017-2587-z</td>\n",
       "      <td>reducing cost preparation claims preparation r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                               DOI  \\\n",
       "0           0  10.1111/j.1745-493X.2012.03267.x   \n",
       "1           1        10.1016/j.ejor.2020.10.016   \n",
       "2           2     10.1057/palgrave.jors.2602125   \n",
       "3           3          10.1108/SCM-06-2015-0244   \n",
       "4           4         10.1007/s10479-017-2587-z   \n",
       "\n",
       "                                                Text  \n",
       "0  chains matter life death increasing number dis...  \n",
       "1  building capacity chains using social vulnerab...  \n",
       "2  blackett memorial lecture logistics management...  \n",
       "3  commentary agility chains provide commentary o...  \n",
       "4  reducing cost preparation claims preparation r...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.to_excel('inputdata_clean.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
